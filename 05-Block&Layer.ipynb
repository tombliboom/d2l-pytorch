{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcec4a4d-0e68-499b-b1af-0005fc9e9192",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 层与块\n",
    "+ 块（block）可以描述单个层、由多个层组成的组件或整个模型本⾝。使⽤块进⾏抽象的⼀个好处是可以将⼀些块组合成更⼤的组件。+ 从编程的⻆度来看，块由类（class）表⽰。它的任何⼦类都必须定义⼀个将其输⼊转换为输出的前向传播函数，并且必须存储任何必需的参数。注意，有些块不需要任何参数。最后，为了计算梯度，块必须具有反向传播函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d64283-15d3-46c6-aa63-b99c0d227eb9",
   "metadata": {},
   "source": [
    "## 1.自定义块\n",
    "+ 每个块必须有的基本功能\n",
    "    + 1. 将输⼊数据作为其前向传播函数的参数。\n",
    "    + 2. 通过前向传播函数来⽣成输出。请注意，输出的形状可能与输⼊的形状不同。例如，我们上⾯模型中的第⼀个全连接的层接收⼀个20维的输⼊，但是返回⼀个维度为256的输出。\n",
    "    + 3. 计算其输出关于输⼊的梯度，可通过其反向传播函数进⾏访问。通常这是⾃动发⽣的。\n",
    "    + 4. 存储和访问前向传播计算所需的参数。\n",
    "    + 5. 根据需要初始化模型参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7daf5410-3427-4a97-8ec5-1e555e2ee26a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b0b570-84c7-446e-9e8d-61941291b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP类继承了表⽰块的类。我们的实现只需要提供我们⾃⼰的构造函数（Python中的__init__函数）和前向传播函数。\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256) # 隐藏层\n",
    "        self.out = nn.Linear(256, 10) # 输出层\n",
    "    \n",
    "    # 定义前向传播\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5afec0-58ad-40b9-b1b2-4034324501ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0273, -0.0458,  0.3154, -0.1051, -0.1642, -0.0974,  0.0413,  0.1286,\n",
       "         -0.0888,  0.1153],\n",
       "        [ 0.0981,  0.0688,  0.2746, -0.1930, -0.0117, -0.1888,  0.2200,  0.2125,\n",
       "         -0.0235,  0.1447]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad875c6c-dec3-4885-88b6-3fc5c9a1fa54",
   "metadata": {},
   "source": [
    "## 2.顺序块\n",
    "+ 需要定义两个关键函数\n",
    "    + 1. ⼀种将块逐个追加到列表中的函数；\n",
    "    + 2. ⼀种前向传播函数，⽤于将输⼊按追加块的顺序传递给块组成的“链条”。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "094f045c-7361-4b63-af43-af03907f4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 个人理解顺序块：就是将每一步计算的步骤类型模块写在了初始化网络时的参数部分，每次如果网络结构有变化，就是在初始网络类的时候变化\n",
    "# 而自定义块的计算步骤就是在类内部就根据需求就写好了，新建模块时内部为黑盒\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里的module就是一个具体的块的实例，定义好后按顺序添加合成一个大的块（网络）\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "    # 前向计算模块\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            # 按照加入的顺序将数据放入这些块中进行计算\n",
    "            X = block(X)\n",
    "        return X\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "113a2a25-7072-4fe1-bf6d-ea828b8689b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0783,  0.0649, -0.0165, -0.1007,  0.0299, -0.2609, -0.0553, -0.2416,\n",
       "          0.0603, -0.1872],\n",
       "        [ 0.1472,  0.0259, -0.0645, -0.0537,  0.0465, -0.1875, -0.1480, -0.1474,\n",
       "          0.0537, -0.1894]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8e716-6742-4af5-b4a4-b484b910c1b3",
   "metadata": {},
   "source": [
    "## 3. 在前向传播函数中执行代码\n",
    "+ Sequential类使模型构造变得简单，允许我们组合新的架构，⽽不必定义⾃⼰的类。然⽽，并不是所有的架构都是简单的顺序架构。当需要更强的灵活性时，我们需要定义⾃⼰的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c704c18b-e0f8-411e-8051-71c029cf5bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数，因此在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "    \n",
    "    # 在前向传播中添加功能\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1) # 矩阵函数\n",
    "        # 复用全连接层，共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流（额外功能）\n",
    "        while X.abs().sum() > 1: \n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d939d-a5e7-4161-ae92-0c9bd9468c1a",
   "metadata": {},
   "source": [
    "## 4.混合块：自定义块与顺序块结合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e8b6b3c-5648-45de-8597-3a05326c5bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3164, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 确定该块的一些输入格式，计算结构\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, 64), # hidden1\n",
    "            nn.ReLU(), # activation1\n",
    "            nn.Linear(64, 32), # hidden2\n",
    "            nn.ReLU(), # activation2\n",
    "        )\n",
    "        self.linear = nn.Linear(32, 16) # fc\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "    \n",
    "chimera = nn.Sequential(NestMLP(),\n",
    "                       nn.Linear(16, 20),\n",
    "                       FixedHiddenMLP())\n",
    "chimera(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755a1cc-c2f3-4276-aa78-2faaa6196af0",
   "metadata": {},
   "source": [
    "## 5. 总结\n",
    "+ ⼀个块可以由许多层组成；⼀个块可以由许多块组成。\n",
    "+ 块可以包含代码。\n",
    "+ 块负责⼤量的内部处理，包括参数初始化和反向传播。\n",
    "+ 层和块的顺序连接由Sequential块处理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691dcbbe-00cb-4cba-8021-5dd5ae5819e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 参数管理\n",
    "+ 访问参数，⽤于调试、诊断和可视化\n",
    "+ 参数初始化\n",
    "+ 在不同模型间共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77f7c75e-9d70-401b-a8f3-00b025b3ec51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4045],\n",
       "        [0.4666]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4259fbad-44b7-4308-af13-bbcd2418c37e",
   "metadata": {},
   "source": [
    "## 1. 参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6439061-c17d-4edd-8d6a-c0c8c78fc41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.2276, -0.1335, -0.2281, -0.1108, -0.0979,  0.3284,  0.3434,  0.0377]])), ('bias', tensor([0.2230]))])\n"
     ]
    }
   ],
   "source": [
    "# 访问最后一个全连接层的参数\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f68c81b-3726-4faa-9632-b08ba0c4cdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.2230], requires_grad=True)\n",
      "tensor([0.2230])\n"
     ]
    }
   ],
   "source": [
    "# 访问目标参数\n",
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d1f0824-9b82-4a07-8e25-07fe2fdf14bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "# 访问网络所有参数\n",
    "# 第一个全连接层\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "# 所有的层\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dc23dff-2991-4159-938e-6620c39493f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4890],\n",
       "        [-0.4890]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从嵌套块收集参数\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                        nn.Linear(8, 4), nn.ReLU())\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bbfc309-5cc4-4504-acd2-6e703b449118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58566e0-3b2f-4acb-b58a-e1e7cbea225c",
   "metadata": {},
   "source": [
    "## 2.参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3264a6e7-dd2e-4637-bff4-b8dd26161565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0119, -0.0009, -0.0129,  0.0005]), tensor(0.))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 内置初始化\n",
    "def init_normal(m):\n",
    "    # 这里m表示某一层的参数\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1ec72bd-c66d-4566-9bbd-4db0adf41425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义初始化\n",
    "# nn.init.uniform_(m.weight, *, *)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50071a4d-49d2-43e3-9d5b-7700c5295c12",
   "metadata": {},
   "source": [
    "## 3. 参数绑定\n",
    "+ 可以定一个共享层，使用其参数来设置另外一个网络中层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "344e2733-b00f-4f42-97dc-e33dd4d993b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                   shared, nn.ReLU(),\n",
    "                   shared, nn.ReLU(),\n",
    "                   nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb1984-ee67-48cd-99e4-417ca0010899",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 延后初始化\n",
    "> 框架的延后初始化（defers initialization），即直到数第⼀次通过模型传递时，框架才会动态地推断出每个层的⼤⼩\n",
    "+ (但是不知道为什么PDF上这一节无代码)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2becf-903f-4449-8ca6-528ba4a58c92",
   "metadata": {},
   "source": [
    "# 自定义层\n",
    "> 我们可以⽤创造性的⽅式组合不同的层，从⽽设计出适⽤于各种任务的架构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de95fc3-0efc-480c-a27c-96cf0bf10c90",
   "metadata": {},
   "source": [
    "## 1. 不带参数的层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd5129c5-6de9-4716-b14d-cb5390d630b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.9849e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "    \n",
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\n",
    "\n",
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a5ee6-722c-4222-9b22-ea7b25f6bbbc",
   "metadata": {},
   "source": [
    "## 2. 带参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2c4a967-3d12-4bdc-98ca-f64f282c892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, inunits, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(inunits, units))\n",
    "        self.bias = nn.Parameter(torch.rand(units, ))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14536267-3643-455d-9c81-fcd53ee1281a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.3969, 0.4178, 0.4165],\n",
       "        [0.2438, 0.4554, 0.4774],\n",
       "        [0.5350, 0.9869, 0.7798],\n",
       "        [0.3520, 0.7862, 0.8062],\n",
       "        [0.0415, 0.5501, 0.4565]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ef9f9c5-a968-41db-b550-34028002d1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9813, 2.0641, 2.4508],\n",
       "        [1.0176, 2.1760, 2.5328]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9f1b0-0369-4227-8c51-d20560856980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0639d9-8c78-4e71-89dd-be90ea3b296d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9918f-56b4-4cdc-a995-467f099bd78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
